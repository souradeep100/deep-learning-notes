# Deep Learning Applications

## Large Scale Deep Learning

### Fast CPU Implementations

- fixed-point arithmetic
- optimizing data structures to avoid cache misses

### GPU Implementations

Video game rendering requires performing many operations in **parallel** quickly. Models of characters and environments are specified in terms of lists of 3-D coordinates of vertices. Graphics cards must perform matrix multiplication and division on many vertices in parallel to convert these <u>3-D coordinates into 2-D</u> on-screen coordinates. The graphics card must then perform many computations at each pixel in parallel to determine the <u>color</u> of each pixel. In both cases, **the computations are fairly simple and do not involve much branching compared to the computational workload that a CPU usually encounters.** For example, each vertex in the same rigid object will be multiplied by the **same matrix**; there is no need to evaluate an if statement per-vertex to determine which matrix to multiply by. The computations are also entirely independent of each other, and thus may be parallelized easily. 

The computations also involve processing <u>massive buffers of memory,</u> containing bitmaps describing the texture (color pattern) of each object to be rendered. 

Together, this results in graphics cards having been designed to have a **high degree of parallelism** and **high memory bandwidth**, at the cost of having a l**ower clock speed** and l**ess branching capability** relative to traditional CPUs.

Neural network algorithms require the same performance characteristics as the real-time graphics algorithms described above. Neural networks usually involve <u>large and numerous buffers of parameters, activation values, and gradient values, each of which must be completely updated during every step of training</u>. These <u>buffers are large enough to fall outside the cache of a traditional desktop computer so the memory bandwidth of the system often becomes the rate limiting factor.</u> GPUs offer a compelling advantage over CPUs due to their **high memory bandwidth**. Neural network training algorithms typically **do not involve much branching** or sophisticated control, so they are appropriate for GPU hardware. Since neural networks can be divided into multiple individual “neurons” that can be processed independently from the other neurons in the same layer, neural networks easily benefit from the **parallelism** of GPU computing.

GPU hardware was originally so specialized that it could only be used for graphics tasks. Over time, GPU hardware became more flexible, allowing custom subroutines to be used to transform the coordinates of vertices or assign colors to pixels. In principle, there was no requirement that these pixel values actually be based on a rendering task. These GPUs could be used for scientific computing by writing the output of a computation to a buffer of pixel values. Steinkrau et al. ( 2005 ) implemented a two-layer fully connected neural network on a GPU and reported a 3X speedup over their CPU-based baseline. Shortly thereafter, Chellapilla et al. ( 2006 ) demonstrated that the same technique could be used to accelerate supervised convolutional networks.

The popularity of graphics cards for neural network training exploded after the advent of general purpose GPUs. These **GP-GPUs** could execute arbitrary code, not just rendering subroutines. **NVIDIA’s CUDA programming language** provided a way to **write this arbitrary code in a C-like language**. <u>With their relatively convenient programming model, massive parallelism, and high memory bandwidth, GP-GPUs now offer an ideal platform for neural network programming.</u> This platform was rapidly adopted by deep learning researchers soon after it became
available ( Raina et al. , 2009 ; Ciresan et al. , 2010 ).

Writing efficient code for GP-GPUs remains a difficult task best left to specialists. The techniques required to obtain good performance on GPU are very different from those used on CPU. For example, good CPU-based code is usually designed to read information from the cache as much as possible. <u>On GPU, most writable memory locations are not cached, so it can actually be faster to compute the same value twice, rather than compute it once and read it back from memory.</u> GPU code is also <u>inherently multi-threaded</u> and the different threads must be coordinated with each other carefully. For example, **memory operations are faster if they can be coalesced**. <u>Coalesced reads or writes occur when several threads can each read or write a value that they need simultaneously, as part of a single memory transaction.</u> Different models of GPUs are able to coalesce different kinds of read or write patterns. Typically, <u>memory operations are easier to coalesce if among n threads, thread i accesses byte i + j of memory, and j is a multiple of some power of 2</u>. The exact specifications differ between models of GPU. Another common consideration for GPUs is making sure that e<u>ach thread in a group executes the same instruction simultaneously</u>. This means that branching can be difficult on GPU. Threads are divided into small groups called **warps** . Each thread in a warp executes the same instruction during each cycle, so if different threads within the same warp need to execute different code paths, these different code paths must be traversed sequentially rather than in parallel.

<u>Due to the difficulty of writing high performance GPU code, researchers should structure their workflow to avoid needing to write new GPU code in order to test new models or algorithms.</u> Typically, one can do this by building a software library of high performance operations like convolution and matrix multiplication, then specifying models in terms of calls to this library of operations. For example, the machine learning library <u>Pylearn2 (Goodfellow et al., 2013c ) specifies all of its machine learning algorithms in terms of calls to Theano</u> ( Bergstra et al. , 2010 ; Bastien et al., 2012 ) <u>and cuda-convnet</u> ( Krizhevsky , 2010 ), which provide these high-performance operations. This factored approach can also ease support for multiple kinds of hardware. For example, the same Theano program can run on
either CPU or GPU, without needing to change any of the calls to Theano itself. Other libraries like **TensorFlow** ( Abadi et al. , 2015 ) and **Torch** ( Collobert et al. , 2011b) provide similar features.

### Large Scale Distributed Implementations

Distributing **inference** is simple, because each input example we want to process can be run by a separate machine. This is known as **data parallelism**.

It is also possible to get **model parallelism**, where multiple machines work together on a single datapoint, with each machine <u>running a different part of the model</u>. This is feasible for <u>both inference and training</u>.

Data parallelism during training is somewhat harder. We can increase the size of the minibatch used for a single SGD step, but usually we get less than linear returns in terms of optimization performance. It would be better to allow multiple machines to compute multiple gradient descent steps in parallel. <u>Unfortunately, the standard definition of gradient descent is as a completely sequential algorithm: the gradient at step t is a function of the parameters produced by step t − 1 .</u>

This can be solved using **asynchronous stochastic gradient descent** (Bengio et al., 2001 ; Recht et al., 2011 ). In this approach, <u>several processor cores share the memory representing the parameters. Each core reads parameters without a lock, then computes a gradient, then increments the parameters without a lock.</u> This reduces the average amount of improvement that each gradient descent step yields, because some of the cores overwrite each other’s progress, but the increased rate of production of steps causes the learning process to be faster overall. Dean et al. ( 2012 ) pioneered the multi-machine implementation of this lock-free approach to gradient descent, where the parameters are managed by a **parameter server** rather than stored in shared memory. <u>**Distributed asynchronous gradient descent remains the primary strategy for training large deep networks and is used by most major deep learning groups in industry**</u> ( Chilimbi et al. , 2014 ; Wu et al., 2015). Academic deep learning researchers typically cannot afford the same scale of distributed learning systems but some research has focused on how to build distributed networks with relatively low-cost hardware available in the university setting ( Coates et al. , 2013 ).

### Model Compression

**In many commercial applications, it is much more important that the time and memory cost of running inference in a machine learning model be low than that the time and memory cost of training be low.** For applications that do not require personalization, it is possible to train a model once, then deploy it to be used by billions of users. In many cases, the end user is more resource-constrained than the developer. For example, one might train a speech recognition network with a powerful computer cluster, then deploy it on **mobile phones.**

A key strategy for reducing the cost of inference is **model compression** (Buciluˇa et al., 2006 ). <u>The basic idea of model compression is to replace the original, expensive model with a smaller model that requires less memory and runtime to store and evaluate.</u>

Model compression is applicable when the size of the original model is driven primarily by a need to prevent overfitting. In most cases, the model with the lowest generalization error is an ensemble of several independently trained models. Evaluating all n ensemble members is expensive. Sometimes, even a single model generalizes better if it is large (for example, if it is regularized with dropout).

These large models learn some function f(x), but do so using many more parameters than are necessary for the task. Their size is necessary only due to the limited number of training examples. As soon as we have fit this function f(x), we can generate a training set containing infinitely many examples, simply by applying f to randomly sampled points x. We then train the new, smaller, model to match f (x) on these points. In order to most efficiently use the capacity of the new, small model, it is best to sample the new x points from a distribution resembling the actual test inputs that will be supplied to the model later. This can be done by corrupting training examples or by drawing points from a generative model trained on the original training set.

Alternatively, one can train the smaller model only on the original training points, but train it to copy other features of the model, such as its posterior distribution over the incorrect classes (Hinton et al., 2014 , 2015 ).

### Dynamic Structure

One strategy for accelerating data processing systems in general is to build systems that have **dynamic structure** in the graph describing the computation needed to process an input. Data processing systems can dynamically determine which subset of many neural networks should be run on a given input. Individual neural networks can also exhibit dynamic structure internally by determining which subset of features (hidden units) to compute given information from the input. This form of dynamic structure inside neural networks is sometimes called **conditional  computation** ( Bengio , 2013 ; Bengio et al. , 2013b ). Since many components of the architecture may be relevant only for a small amount of possible inputs, the system can run faster by computing these features only when they are needed.

Dynamic structure of computations is a basic computer science principle applied generally throughout the software engineering discipline. <u>The simplest versions of dynamic structure applied to neural networks are based on determining which subset of some group of neural networks (or other machine learning models) should be applied to a particular input.</u>

A venerable strategy for accelerating inference in a classifier is to use a **cascade of classifiers**. The cascade strategy may be applied when the goal is to detect the presence of a rare object (or event). To know for sure that the object is present, we must use a sophisticated classifier with high capacity, that is expensive to run. However, because the object is rare, we can usually use much less computation to reject inputs as not containing the object. In these situations, we can train a sequence of classifiers. The first classifiers in the sequence have low capacity, and are trained to have high recall. In other words, they are trained to make sure we do not wrongly reject an input when the object is present. The final classifier is trained to have high precision. At test time, we run inference by running the classifiers in a sequence, abandoning any example as soon as any one element in the cascade rejects it. Overall, this allows us to verify the presence of objects with high confidence, using a high capacity model, but does not force us to pay the cost of full inference for every example. There are two different ways that the cascade can achieve high capacity. One way is to make the later members of the cascade individually have high capacity. In this case, the system as a whole obviously has high capacity, because some of its individual members do. It is also possible to make a cascade in which every individual model has low capacity but the system as a whole has high capacity due to the combination of many small models. Viola and Jones ( 2001 ) used a cascade of boosted decision trees to implement a fast and robust face detector suitable for use in handheld digital cameras. Their classifier localizes a face using essentially a sliding window approach in which many windows are examined and rejected if they do not contain faces. Another version of cascades uses the earlier models to implement a sort of hard attention mechanism: the early members of the cascade localize an object and later members of the cascade perform further processing given the location of the object. For example, Google transcribes address numbers from Street View imagery using a two-step cascade that first locates the address number with one machine learning model and then transcribes it with another (Goodfellow et al., 2014d ).

<u>Decision trees themselves are an example of dynamic structure</u>, because each node in the tree determines which of its subtrees should be evaluated for each input. A simple way to accomplish the **union of deep learning and dynamic structure** is to train a decision tree in which each node uses a neural network to make the splitting decision ( Guo and Gelfand , 1992 ), though this has typically not been done with the primary goal of accelerating inference computations.

In the same spirit, one can use a neural network, called the gater to select which one out of several expert networks will be used to compute the output, given the current input. The first version of this idea is called the mixture of experts ( Nowlan , 1990 ; Jacobs et al., 1991 ), in which the gater outputs a set of probabilities or weights (obtained via a softmax nonlinearity), one per expert, and the final output is obtained by the weighted combination of the output of the experts. In that case, the use of the gater does not offer a reduction in computational cost, but if a single expert is chosen by the gater for each example, we obtain the hard mixture of experts ( Collobert et al. , 2001 , 2002 ), which can considerably accelerate training and inference time. This strategy works well when the number of gating decisions is small because it is not combinatorial. But when we want to select different subsets of units or parameters, it is not possible to use a “soft switch” because it requires enumerating (and computing outputs for) all the gater configurations. To deal with this problem, several approaches have been explored to train combinatorial gaters. Bengio et al. ( 2013b ) experiment with several estimators of the gradient on the gating probabilities, while Bacon et al. ( 2015 ) and Bengio et al. ( 2015a ) use reinforcement learning techniques (policy gradient) to learn a form of conditional dropout on blocks of hidden units and get an actual reduction in computational cost without impacting negatively on the quality of the approximation.

Another kind of dynamic structure is a **switch**, where a hidden unit can receive input from different units depending on the context. This dynamic routing approach can be interpreted as an attention mechanism ( Olshausen et al. , 1993 ). So far, the use of a hard switch has not proven effective on large-scale applications. Contemporary approaches instead use a weighted average over many possible inputs, and thus do not achieve all of the possible computational benefits of dynamic structure. Contemporary attention mechanisms are described in Sec. 12.4.5.1 .

One major obstacle to using dynamically structured systems is the decreased degree of parallelism that results from the system following different code branches for different inputs. This means that few operations in the network can be described as matrix multiplication or batch convolution on a minibatch of examples. We can write more specialized sub-routines that convolve each example with different kernels or multiply each row of a design matrix by a different set of columns of weights. Unfortunately, these more specialized subroutines are difficult to implement efficiently. CPU implementations will be slow due to the lack of cache coherence and GPU implementations will be slow due to the lack of coalesced memory transactions and the need to serialize warps when members of a warp take different branches. In some cases, these issues can be mitigated by partitioning the examples into groups that all take the same branch, and processing these groups of examples simultaneously. This can be an acceptable strategy for minimizing the time required to process a fixed amount of examples in an offline setting. In a real-time setting where examples must be processed continuously, partitioning the workload can result in load-balancing issues. For example, if we assign one machine to process the first step in a cascade and another machine to process
the last step in a cascade, then the first will tend to be overloaded and the last will tend to be underloaded. Similar issues arise if each machine is assigned to implement different nodes of a neural decision tree.

### Specialized Hardware Implementations of Deep Networks

Since the early days of neural networks research, hardware designers have worked on specialized hardware implementations that could speed up training and/or inference of neural network algorithms. See early and more recent reviews of specialized hardware for deep networks ( Lindsey and Lindblad , 1994 ; Beiu et al. ,
2003 ; Misra and Saha , 2010 ).

Different forms of specialized hardware (Graf and Jackel , 1989 ; Mead and Ismail , 2012 ; Kim et al., 2009 ; Pham et al., 2012 ; Chen et al., 2014a , b ) have been developed over the last decades, either with ASICs (application-specific integrated circuit), either with digital (based on binary representations of numbers), analog (Graf and Jackel , 1989 ; Mead and Ismail , 2012 ) (based on physical implementations of continuous values as voltages or currents) or hybrid implementations (combining digital and analog components). I**n recent years more flexible FPGA (field programmable gated array) implementations (where the particulars of the circuit can be written on the chip after it has been built) have been developed.**

Though software implementations on general-purpose processing units (CPUs and GPUs) typically use 32 or 64 bits of precision to represent floating point numbers, **it has long been known that it was possible to use less precision, at least at inference time** (Holt and Baker , 1991 ; Holi and Hwang , 1993 ; Presley and Haggard , 1994 ; Simard and Graf , 1994 ; Wawrzynek et al., 1996 ; Savich et al., 2007). This has become a more pressing issue in recent years as deep learning has gained in popularity in industrial products, and as the great impact of faster hardware was demonstrated with GPUs. Another factor that motivates current research on specialized hardware for deep networks is that the rate of progress of a single CPU or GPU core has slowed down, and most recent improvements in computing speed have **come from parallelization across cores** (either in CPUs or GPUs). This is very different from the situation of the 1990s (the previous neural network era) where the hardware implementations of neural networks (which mighttake two years from inception to availability of a chip) could not keep up with the rapid progress and low prices of general-purpose CPUs. Building specialized hardware is thus a way to push the envelope further, at a time when new hardware designs are being developed for low-power devices such as phones, aiming for general-public applications of deep learning (e.g., with speech, computer vision or natural language).

Recent work on low-precision implementations of backprop-based neural nets (Vanhoucke et al., 2011 ; Courbariaux et al., 2015 ; Gupta et al., 2015 ) suggests that between 8 and 16 bits of precision can suffice for using or training deep
neural networks with back-propagation. What is clear is that more precision is required during training than at inference time, and that some forms of dynamic fixed point representation of numbers can be used to reduce how many bits are required per number. Traditional fixed point numbers are restricted to a fixed range (which corresponds to a given exponent in a floating point representation). **Dynamic fixed point representations** share that range among a set of numbers (such as all the weights in one layer). Using fixed point rather than floating point
representations and using less bits per number reduces the hardware surface area, power requirements and computing time needed for performing multiplications, and multiplications are the most demanding of the operations needed to use or train a modern deep network with backprop.